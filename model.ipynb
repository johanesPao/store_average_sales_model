{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales Average Model\n",
    "---\n",
    "Notebook ini merupakan studi mengenai hubungan antara Jenis Bangunan, Kelas Bangunan, Lokasi Kabupaten/Kota dan Luas Area Toko dalam men-generate penjualan per bulan di PT Prestasi Retail Innovation.\n",
    "- Jenis Bangunan: Merupakan satu dari tiga tipe bangunan diantaranya Mall (M), Ruko (R) dan Bangunan Sendiri (S)\n",
    "- Kelas Bangunan: Merupakan kelas dari tipe bangunan tersebut dalam men-generate sales. Misalkan Mall Grand Indonesia adalah Mall Kelas 1 (M1) sedangkan Gandaria City adalah Mall Kelas 4 (M4). Berikut adalah Kelas Bangunan yang ada.\n",
    "  - Mall Kelas 1 (M1)\n",
    "  - Mall Kelas 2 (M2)\n",
    "  - Mall Kelas 3 (M3)\n",
    "  - Mall Kelas 4 (M4)\n",
    "  - Mall Kelas 5 (M5)\n",
    "  - Ruko Kelas 1 (R1)\n",
    "  - Ruko Kelas 2 (R2)\n",
    "  - Ruko Kelas 3 (R3)\n",
    "  - Ruko Kelas 4 (R4)\n",
    "  - Ruko Kelas 5 (R5)\n",
    "  - Bangunan Sendiri Kelas 1 (S1)\n",
    "  - Bangunan Sendiri Kelas 2 (S2)\n",
    "  - Bangunan Sendiri Kelas 3 (S3)\n",
    "  - Bangunan Sendiri Kelas 4 (S4)\n",
    "  - Bangunan Sendiri Kelas 5 (S5)\n",
    "- Lokasi Kabupaten/Kota adalah lokasi geografis dari toko\n",
    "- Luas Area Toko adalah luas meter persegi dari toko\n",
    "---\n",
    "Studi ini akan menggunakan Regresi Linear dalam merumuskan nilai Average Sales ($y$) yang dipengaruhi oleh variabel - variabel independen lainnya seperti Jenis Bangunan $x{_1}$, Kelas Bangunan $x{_2}$, Lokasi Kabupaten/Kota ($x{_3}$) dan Luas Area Toko ($x{_4}$). Berikut adalah formulasi Regresi Linear untuk permasalahan tersebut:  \n",
    "  \n",
    "$\n",
    "y = ax{_1} + bx{_2} + cx{_3} + dx{_4} + e\n",
    "$  \n",
    "  \n",
    "Dimana:  \n",
    "$y$ = Prediksi Average Sales  \n",
    "$x{_1}$ = Variabel independen mewakili Jenis Bangunan  \n",
    "$x{_2}$ = Variabel independen mewakili Kelas Bangunan  \n",
    "$x{_3}$ = Variabel independen mewakili Lokasi Kabupaten/Kota  \n",
    "$x{_4}$ = Variabel independen mewakili Luas Area Toko  \n",
    "$a$ = Koefisien variabel independen $x{_1}$  \n",
    "$b$ = Koefisien variabel independen $x{_2}$  \n",
    "$c$ = Koefisien variabel independen $x{_3}$  \n",
    "$d$ = Koefisien variabel independen $x{_4}$  \n",
    "$e$ = Bias dari Regresi Linear  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(11)\n",
    "from numpy.random import seed\n",
    "seed(11)\n",
    "from tensorflow.random import set_seed # type: ignore\n",
    "set_seed(11)\n",
    "import os\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from joblib import dump\n",
    "import lightgbm\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versi Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Tampilan DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksplorasi Data\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atribut Dasar Toko\n",
    "Berikut adalah beberapa data atribut dasar toko yang saat ini dimiliki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_toko = pd.read_excel(\"PRI - Store Renov Rent.xlsx\", sheet_name=0, header=0)\n",
    "data_toko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena kita tidak akan menggunakan semua kolom dalam data ini untuk kepentingan studi Store Sales Average, maka `data_toko` akan diringkas dan disusun ulang menjadi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_toko = data_toko[[\"STORE CODE\", \"STORE NAME\", \"Tipe Bangunan\", \"Kelas Bangunan\", \"Kota Kabupaten 2\", \"Estimasi Populasi\", \"sqm\"]]\n",
    "data_toko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luas Area Toko (sqm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_luas = data_toko[\"sqm\"]\n",
    "hitung, bin = np.histogram(data_luas)\n",
    "print(hitung, bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = so.Plot(data_toko, \"sqm\")\n",
    "plot.add(so.Bars(), so.Hist(), legend=True).label(title=\"Persebaran Toko berdasar SQM\", x=\"Square Meters (sqm)\", y=\"Jumlah Toko\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan luasnya (sqm), kita dapat melihat pada fungsi `histogram` di atas bahwa distribusi persebaran luas toko cukup normal dengan 29 Toko jatuh ke dalam kategori `sqm` di antara $75m{^2}$ sampai dengan $150m{^2}$, dengan 1 toko yang menjadi outlier dari distribusi dimana luas toko > $400m{^2}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjualan\n",
    "---\n",
    "Mengingat bahwa data *historical* yang dimiliki terbatas dari tahun 2018 sampai dengan November 2022, serta mengingat bahwa kita mengalami periode pandemi CoV-19 selama lebih dari 1 tahun, maka penulis merasa perlu untuk melakukan separasi data penjualan per bulan menggunakan flag `Pandemic`.\n",
    "  \n",
    "Berikut adalah sepenggal data penjualan *historical* per toko dari tahun 2018 sampai dengan November 2022 (40 baris data awal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_penjualan = pd.read_excel(\"PRI - Store Renov Rent.xlsx\", sheet_name=\"Sales\", header=0)\n",
    "data_penjualan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah grouping data penjualan *historical* yang disimpan dalam variabel `data_penjualan_by_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_penjualan_by_month = data_penjualan.groupby([\"EOM\"]).sum(numeric_only=True)\n",
    "data_penjualan_by_month"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melihat trend pergerakan penjualan dalam kurun waktu ini, kita akan menggunakan `Simple Moving Average` yang akan menghitung rerata secara bergulung untuk interval waktu ke belakang (contoh: Moving Average 3 Bulan untuk Mei 2021 adalah rata - rata penjualan yang merupakan rata - rata dari penjualan di bulan Maret 2021, April 2021 dan Mei 2021).  \n",
    "Namun satu `Moving Average` saja tidak dapat menggambarkan sebuah trend, karena itu kita juga akan menggunakan tambahan 2 interval `Moving Average` lainnya untuk menggambarkan trend pada jangka pendek, jangka menengah dan jangka panjang.  \n",
    "Dalam studi ini kita akan menggunakan 3 `Simple Moving Average` yaitu:\n",
    "* `MA3`: `Moving Average` dengan jendela periode 3 bulan ke belakang (Jangka Pendek)\n",
    "* `MA6`: `Moving Average` dengan jendela periode 6 bulan ke belakang (Jangka Menengah)\n",
    "* `MA12`: `Moving Average` dengan jendela periode 12 bulan atau 1 tahun ke belakang (Jangka Panjang)  \n",
    "Berikut adalah `data_penjualan_by_month` dengan penambahan kolom `MA3`, `MA6` dan `MA12` yang didapat dengan menggunakan fungsi `rolling()` dari `pd.DataFrame` yang dirata-ratakan dengan fungsi `mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_penjualan_by_month[\"MA3\"] = data_penjualan_by_month[\"Sales\"].rolling(3).mean()\n",
    "data_penjualan_by_month[\"MA6\"] = data_penjualan_by_month[\"Sales\"].rolling(6).mean()\n",
    "data_penjualan_by_month[\"MA12\"] = data_penjualan_by_month[\"Sales\"].rolling(12).mean()\n",
    "data_penjualan_by_month"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah grafik penjualan *historical* dari tahun 2018 sampai dengan November 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garis_plot = so.Plot(data_penjualan_by_month, \"EOM\", \"Sales\")\n",
    "garis_plot.add(so.Line()).label(title=\"Total Penjualan per Bulan (2018 - Nov 2022)\", x=\"Tahun\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah grafik `Moving Average` untuk data penjualan *historical* dari tahun 2018 sampai dengan November 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_penjualan_by_month.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kolom_melt = pd.melt(data_penjualan_by_month.reset_index().drop(columns=[\"Sales\"]), id_vars='EOM', var_name=\"Tipe MA\", value_name=\"Nilai\") # type: ignore\n",
    "data_kolom_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(data_penjualan_by_month[\"MA3\"], color=\"green\", label=\"MA3\")\n",
    "# plt.plot(data_penjualan_by_month[\"MA6\"], color=\"orange\", label=\"MA6\")\n",
    "# plt.plot(data_penjualan_by_month[\"MA12\"], color=\"red\", label=\"MA12\")\n",
    "# plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Moving Average Total Penjualan per Bulan\")\n",
    "ax.set_ylabel(\"Rata-Rata Total Penjualan per Bulan\")\n",
    "ax.set_xlabel(\"Tahun\")\n",
    "# Plot Moving Average\n",
    "sns.lineplot(data_kolom_melt, x=\"EOM\", y=\"Nilai\", hue=\"Tipe MA\")\n",
    "# Region Section Pandemic\n",
    "ax.fill_between(data_penjualan_by_month.index.values, 0, 28000000000, where=((data_penjualan_by_month.index.values > np.datetime64('2020-02-29')) & (data_penjualan_by_month.index.values <= np.datetime64('2021-10-31'))), color=\"red\", alpha=0.2)\n",
    "# Region Section Recovery\n",
    "ax.fill_between(data_penjualan_by_month.index.values, 0, 28000000000, where=((data_penjualan_by_month.index.values >= np.datetime64('2021-11-01')) & (data_penjualan_by_month.index.values <= np.datetime64('2022-12-31'))), color=\"green\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada grafik di atas kita dapat melihat bahwa terdapat tren penurunan penjualan (terkonfirmasi dengan `MA3` yang turun ke bawah `MA6` dan `MA12`) pada periode Maret 2020 (`Pandemic`) dan nilai rata - rata penjualan ini bertahan cukup rendah hingga setidaknya sampai dengan bulan Oktober 2021 dan di bulan November 2021 hingga seterusnya kita dapat melihat nilai rata-rata penjualan per bulan yang meningkat (`Recovery`, terkonfirmasi dengan `MA3` yang melewati dan bertahan di atas `MA6` dan `MA12`).  \n",
    "  \n",
    "Oleh karena itu kita akan mengkategorikan penjualan yang terjadi diantara bulan Maret 2020 - Oktober 2021 sebagai penjualan dalam masa `Pandemic` dan lainnya sebagai penjualan `Normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandemic_period = [\n",
    "  np.datetime64('2020-03-31'),\n",
    "  np.datetime64('2020-04-30'),\n",
    "  np.datetime64('2020-05-31'),\n",
    "  np.datetime64('2020-06-30'),\n",
    "  np.datetime64('2020-07-31'),\n",
    "  np.datetime64('2020-08-31'),\n",
    "  np.datetime64('2020-09-30'),\n",
    "  np.datetime64('2020-10-31'),\n",
    "  np.datetime64('2020-11-30'),\n",
    "  np.datetime64('2020-12-31'),\n",
    "  np.datetime64('2021-01-31'),\n",
    "  np.datetime64('2021-02-28'),\n",
    "  np.datetime64('2021-03-31'),\n",
    "  np.datetime64('2021-04-30'),\n",
    "  np.datetime64('2021-05-31'),\n",
    "  np.datetime64('2021-06-30'),\n",
    "  np.datetime64('2021-07-31'),\n",
    "  np.datetime64('2021-08-31'),\n",
    "  np.datetime64('2021-09-30'),\n",
    "  np.datetime64('2021-10-31'),\n",
    "  ]\n",
    "print(pandemic_period)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah pengkategorian bulan penjualan berdasarkan periode `Pandemic` dan `Normal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_pandemi(x):\n",
    "  return \"Pandemic\" if x[\"EOM\"] in pandemic_period else \"Normal\"\n",
    "\n",
    "data_penjualan[\"Status Pandemi\"] = data_penjualan.apply(lambda x: status_pandemi(x), axis=1)\n",
    "data_penjualan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembentukan dataframe `data_penjualan_rerata` untuk lookup nilai penjualan rata-rata pada masa pandemi dan normal di dataframe `data_toko`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_penjualan_rerata = data_penjualan.groupby(['Status Pandemi', 'LocationCode']).mean(numeric_only=True)\n",
    "data_penjualan_rerata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementasi lookup rata-rata penjualan per bulan untuk setiap toko baik pada masa pandemi maupun pada masa normal di dataframe `data_toko`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rerata_penjualan = data_toko.copy()\n",
    "\n",
    "def lookup_rerata(x, pandemi_status, lookup_df):\n",
    "  try: \n",
    "    return sum(lookup_df.loc[pandemi_status, x['STORE CODE']].values)\n",
    "  except:\n",
    "    return np.NaN\n",
    "\n",
    "data_rerata_penjualan[\"Rerata Penjualan Normal\"] = data_rerata_penjualan.apply(lambda x: lookup_rerata(x, 'Normal', data_penjualan_rerata), axis=1) # type: ignore\n",
    "data_rerata_penjualan[\"Rerata Penjualan Pandemi\"] = data_rerata_penjualan.apply(lambda x: lookup_rerata(x, 'Pandemic', data_penjualan_rerata), axis=1) # type: ignore\n",
    "\n",
    "data_rerata_penjualan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada dataframe `data_rerata_penjualan` dengan penambahan kolom `Rerata Penjualan Normal` dan `Rerata Penjualan Pandemi` kita dapat melihat bahwa nilai `Rerata Penjualan Normal` lebih besar daripada nilai `Rerata Penjualan Pandemi` untuk kesemua toko, hal ini menunjukkan bahwa kita berhasil menangkap nilai rata-rata penjualan per bulan di masa normal yang kita ekspektasikan menjadi acuan ke depannya.\n",
    "\n",
    "Nilai pada kolom `Rerata Penjualan Normal` ini adalah nilai $y$ yang sebenarnya. Nilai $y$ yang sebenarnya ini akan menjadi acuan dalam proses pelatihan jaringan saraf tiruan untuk melihat seberapa akurat model dalam memprediksi nilai $y$ atau yang kita sebut $\\hat{y}$ (*y-hat* atau prediksi y)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STORE CODE & KOTA KABUPATEN 2\n",
    "---\n",
    "Dalam membangun model prediksi, selain mempertimbangkan input dalam proses pelatihan model, kita juga harus mempertimbangkan interaksi pengguna dengan model nantinya dalam menghasilkan prediksi rata-rata penjualan per bulan.\n",
    "Jika kita membayangkan pengguna melakukan input pada serangkaian form untuk mendapatkan nilai output prediksi rata-rata penjualan per bulan untuk input yang diberikan, nampaknya akan sulit jika pengguna menginput semisalkan `STORE CODE` 'FS040' atau `KOTA KABUPATEN 2` 'PALU'. Hal ini dikarenakan model akan dilatih menggunakan data pada `data_toko` yang jumlah sampelnya sangat terbatas dan tidak pernah mengenal 'FS040' atau 'PALU' sebagai salah satu input dalam proses pelatihan model.  \n",
    "Oleh karena itu, kita akan melakukan modifikasi pada kedua variabel ini untuk memastikan proses pelatihan berjalan lebih umum (*general*) dan untuk memungkinkan input oleh pengguna pada model nantinya lebih umum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STORE CODE\n",
    "Untuk `STORE CODE`, supaya baik proses pelatihan maupun input pada model nantinya bisa berlaku secara lebih umum, kita akan menggunakan `SBU` yang diekstrak dari dua karakter pertama dalam `STORE CODE` dan untuk FO akan masuk ke dalam `SBU` 'Fisik Sport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store_code = data_rerata_penjualan.copy()\n",
    "\n",
    "def konversi_sbu(x):\n",
    "  try:\n",
    "    match x['STORE CODE'][:2]:\n",
    "      case \"FS\" | \"FO\":\n",
    "        return \"Fisik Sport\"\n",
    "      case \"FF\":\n",
    "        return \"Fisik Football\"\n",
    "      case \"OD\":\n",
    "        return \"Our Daily Dose\"\n",
    "      case _:\n",
    "        return np.NaN\n",
    "  except:\n",
    "    return np.NaN\n",
    "\n",
    "data_store_code[\"SBU\"] = data_store_code.apply(lambda x: konversi_sbu(x), axis=1) # type: ignore\n",
    "\n",
    "# Reorder kolom\n",
    "kolom = [\"STORE CODE\", \"STORE NAME\", \"SBU\", \"Tipe Bangunan\", \"Kelas Bangunan\", \"Kota Kabupaten 2\", \"Estimasi Populasi\", \"sqm\", \"Rerata Penjualan Normal\", \"Rerata Penjualan Pandemi\"]\n",
    "data_store_code = data_store_code[kolom]\n",
    "  \n",
    "data_store_code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KABUPATEN KOTA 2\n",
    "Untuk `KABUPATEN KOTA 2`, kita akan melakukan grouping rentang populasi, misalkan populasi `0 - 500,000`, `500,001 - 1,000,000` dstnya. Hal ini dipandang lebih baik untuk proses pelatihan jaringan saraf tiruan model dan juga untuk implementasi prediksi model pada aplikasi ke depannya, mengingat jumlah sampel pelatihan yang sangat terbatas.  \n",
    "Sebelumnya, dipandang perlu untuk melihat kardinalitas anggota dalam rentang yang terbentuk untuk memastikan distribusi yang mendekati normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlah_anggota, bin = np.histogram(data_store_code[\"Estimasi Populasi\"], bins=6, range=(0, 3000000))\n",
    "print(f\"Kardinalitas anggota: \\t{jumlah_anggota}\")\n",
    "print(f\"Range Bin: \\t\\t{bin}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada fungsi `histogram()` di atas kita mengelompokkan data `Estimasi Populasi` ke dalam 6 rentang dengan nilai rentang minimal dimulai dari 0 dan nilai rentang maksimal sebesar 3,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"Persebaran Rentang Populasi Toko\")\n",
    "ax.hist(bin[:-1], weights=jumlah_anggota, range=(0, 3000000))\n",
    "ax.set_ylabel(\"Jumlah Toko di Kota dengan Rentang Populasi\")\n",
    "ax.set_xlabel(\"Rentang Populasi\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada grafik histogram di atas kita dapat melihat bahwa persebaran data cukup normal dimana sebagian besar toko dibuka di kota dengan populasi `1,000,000 - 1,500,000` (10 Toko) dan `1,500,001 - 2,000,000` (12 Toko) penduduk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rentang_populasi = data_store_code.copy()\n",
    "\n",
    "def konversi_rentang_populasi(x):\n",
    "  try:\n",
    "    match x['Estimasi Populasi']:\n",
    "      case x if x <= 500000:\n",
    "        return '0 - 500000'\n",
    "      case x if x <= 1000000:\n",
    "        return '500001 - 1000000'\n",
    "      case x if x <= 1500000:\n",
    "        return '1000001 - 1500000'\n",
    "      case x if x <= 2000000:\n",
    "        return '1500001 - 2000000'\n",
    "      case x if x <= 2500000:\n",
    "        return '2000001 - 2500000'\n",
    "      case _:\n",
    "        return '> 2500000'\n",
    "  except:\n",
    "    return\n",
    "\n",
    "# data_toko['Rentang Populasi'] = data_toko.apply(lambda x: konversi_rentang_populasi(x), axis=1) # type: ignore\n",
    "data_rentang_populasi['Rentang Populasi'] = data_rentang_populasi.copy().apply(konversi_rentang_populasi, axis=1) # type: ignore\n",
    "\n",
    "# Reorder kolom\n",
    "kolom = [\"STORE CODE\", \"STORE NAME\", \"SBU\", \"Tipe Bangunan\", \"Kelas Bangunan\", \"Kota Kabupaten 2\", \"Estimasi Populasi\", \"Rentang Populasi\", \"sqm\", \"Rerata Penjualan Normal\", \"Rerata Penjualan Pandemi\"]\n",
    "data_rentang_populasi = data_rentang_populasi[kolom]\n",
    "\n",
    "data_rentang_populasi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persebaran Data Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_rentang_populasi[\"SBU\"].value_counts())\n",
    "data_rentang_populasi[\"SBU\"].value_counts().plot(kind=\"bar\", figsize=(3, 3), title=\"Persebaran Data SBU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persebaran data untuk jenis SBU masih bisa dianggap cukup normal dan tidak memiliki outlier maupun *skewness* yang signigikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_rentang_populasi[\"Kelas Bangunan\"].value_counts())\n",
    "data_rentang_populasi[\"Kelas Bangunan\"].value_counts().sort_index().plot(kind=\"bar\", figsize=(3, 3), title=\"Persebaran Data Kelas Bangunan\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persebaran data untuk Kelas Bangunan seperti nampak diatas sekilas cukup normal. Namun jika kita hanya memperhitungkan Kelas Bangunan dalam kategori Mall, hanya ada 1 baris data yang masuk ke dalam kategori M5. Hal yang sama juga terjadi pada kategori Kelas Bangunan S5 yang hanya memiliki 1 anggota saja di dalam kategorinya. Untuk itu kita akan melakukan *feature engineering* untuk kolom SBU ini nantinya untuk mencapai distribusi data yang lebih normal dan merata dalam model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_rentang_populasi[\"Rentang Populasi\"].value_counts())\n",
    "list_rentang_populasi = [\n",
    "  \"0 - 500000\", \n",
    "  \"500001 - 1000000\", \n",
    "  \"1000001 - 1500000\", \n",
    "  \"1500001 - 2000000\", \n",
    "  \"2000001 - 2500000\", \n",
    "  \"> 2500000\"\n",
    "]\n",
    "data_rentang_populasi[\"Rentang Populasi\"].value_counts().reindex(list_rentang_populasi).plot(kind=\"bar\", figsize=(3,3), title=\"Persebaran Data Rentang Populasi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persebaran data untuk Rentang Populasi terlihat cukup normal meski memiliki kecenderungan (*skewness*) di sisi kanan."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering untuk Kelas Bangunan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan temuan pada bagian sebelumnya, kita akan melakukan *feature engineering* pada fitur Kelas Bangunan untuk mendapatkan distribusi kategori yang lebih normal dan kardinalitas anggota yang lebih baik untuk proses pelatihan nantinya.\n",
    "\n",
    "Dengan asumsi sebagian besar pembukaan toko baru akan dilakukan di Mall dibandingkan dengan bangunan yang berdiri sendiri dan dengan mereduksi kategori Mall menjadi 3 kelas saja, maka kita akan melakukan mapping sebagai berikut:\n",
    "- M1 tetap menjadi M1\n",
    "- M2 dan M3 menjadi M2\n",
    "- M4 tetap menjadi M3\n",
    "- M5, R5 dan S5 akan menjadi M4NM (Mall 4/Non-Mall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = data_rentang_populasi.copy()\n",
    "\n",
    "def feature_engineering_kb(x):\n",
    "  match x[\"Kelas Bangunan\"]:\n",
    "    case \"M1\":\n",
    "      return \"M1\"\n",
    "    case \"M2\" | \"M3\":\n",
    "      return \"M2\"\n",
    "    case \"M4\":\n",
    "      return \"M3\"\n",
    "    case _:\n",
    "      return \"M4NM\"\n",
    "\n",
    "data_prep[\"Kelas Bangunan FE\"] = data_prep.apply(lambda x: feature_engineering_kb(x), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persebaran data Kelas Bangunan FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep[\"Kelas Bangunan FE\"].value_counts()\n",
    "data_prep[\"Kelas Bangunan FE\"].value_counts().sort_index().plot(kind=\"bar\", figsize=(3, 3), title=\"Persebaran Data Kelas Bangunan FE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konversi Data Categorical ke dalam Label Encoder\n",
    "---\n",
    "Dengan perubahan-perubahan pada sub-bagian sebelumnya maka fungsi regresi linear dapat digambarkan ulang sebagai berikut:  \n",
    "  \n",
    "$\n",
    "{Rerata Penjualan Normal} = a \\cdot {SBU} + b \\cdot {Kelas Bangunan FE} + c \\cdot {Luas Area} + d \\cdot {Rentang Populasi} + e\n",
    "$ \n",
    "   \n",
    "Dikarenakan `SBU`, `Kelas Bangunan FE` dan `Rentang Populasi` merupakan tipe data *categorical*, sedangkan pelatihan jaringan saraf tiruan untuk sebuah model memerlukan semua input dalam bentuk numerik, maka kita akan melakukan konversi pada ketiga data tersebut menjadi numerik.  \n",
    "Dilihat dari jenis datanya, `SBU` dapat kita kategorikan sebagai *categorical nominal*, sedangkan `Rentang Populasi` dan `Kelas Kategori FE` merupakan *categorical ordinal*.  \n",
    "Untuk data *categorical nominal* kita akan menerapkan proses *One Hot Encoding* untuk menerapakan pelabelan numerik tanpa susunan maupun bobot dan untuk data *categorical ordinal* kita akan menggunakan *Ordinal Encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding SBU\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "sbu_encoded = ohe.fit_transform(data_prep[\"SBU\"].values.reshape(-1, 1)) # type: ignore\n",
    "data_sbu_encoded = pd.DataFrame(sbu_encoded, columns=ohe.get_feature_names_out([\"SBU\"]))\n",
    "print(f\"Data SBU setelah proses One Hot Encoding: \\n{data_sbu_encoded}\")\n",
    "\n",
    "# Ordinal Encoding Kelas Bangunan dan Rentang Populasi\n",
    "oe = OrdinalEncoder()\n",
    "kelas_bangunan_fe_encoded = oe.fit_transform(data_prep[\"Kelas Bangunan FE\"].values.reshape(-1, 1)) # type: ignore\n",
    "data_kelas_bangunan_fe_encoded = pd.DataFrame(kelas_bangunan_fe_encoded, columns=[\"Kelas Bangunan FE Encoded\"])\n",
    "print(f\"Data Kelas Bangunan FE setelah proses Ordinal Encoding: \\n{data_kelas_bangunan_fe_encoded}\")\n",
    "oe = OrdinalEncoder(categories=[list_rentang_populasi])\n",
    "rentang_populasi_encoded = oe.fit_transform(data_prep[\"Rentang Populasi\"].values.reshape(-1, 1)) # type: ignore\n",
    "data_rentang_populasi_encoded = pd.DataFrame(rentang_populasi_encoded, columns=[\"Rentang Populasi Encoded\"])\n",
    "print(f\"Data Rentang Populasi setelah proses Ordinal Encoding: \\n{data_rentang_populasi_encoded}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pemilihan fitur dan label untuk bahan pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_full = pd.concat([data_prep, data_sbu_encoded, data_kelas_bangunan_fe_encoded, data_rentang_populasi_encoded], axis=1)\n",
    "data_model_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_model_full.copy()\n",
    "data_final = data_final.drop(columns=[\"STORE CODE\", \"STORE NAME\", \"SBU\", \"Tipe Bangunan\", \"Kelas Bangunan\", \"Kelas Bangunan FE\", \"Kota Kabupaten 2\", \"Estimasi Populasi\", \"Rentang Populasi\", \"Rerata Penjualan Pandemi\"], axis=1)\n",
    "data_final = data_final[[\"sqm\", \"SBU_Fisik Football\", \"SBU_Fisik Sport\", \"SBU_Our Daily Dose\", \"Kelas Bangunan FE Encoded\", \"Rentang Populasi Encoded\", \"Rerata Penjualan Normal\"]]\n",
    "data_final.dropna(subset=[\"Rerata Penjualan Normal\"], inplace=True)\n",
    "data_final.reset_index(drop=True, inplace=True)\n",
    "data_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksplorasi Data Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koefisien_korelasi = data_final.corr()\n",
    "koefisien_korelasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(koefisien_korelasi, annot=True, cmap='magma')\n",
    "plt.title(\"Heatmap Pearson Correlation Data Final\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan berikut adalah distribusi variabel independen terkait dengan variable independen lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.pairplot(data=data_final[[\"sqm\", \"Kelas Bangunan FE Encoded\", \"Rentang Populasi Encoded\"]], diag_kind='kde')\n",
    "plot.fig.suptitle(\"Distribusi antara Variabel Independen\", y=1.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling dan Pembentukan Data Train Test\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Merujuk kepada nilai dalam data yang dimiliki, kita sebenarnya hanya memiliki 1 fitur (kolom) dengan nilai numerik, yaitu kolom `sqm` sedangkan sisanya merupakan kategori yang di-encode baik secara One Hot Encoding maupun Label Encoding. Hal ini menyebabkan *mean* dari `sqm` memiliki nilai yang jauh berbeda dengan *mean* fitur - fitur lainnya, dan untuk mencegah proses update bobot dalam layer dari jaringan saraf tiruan dalam proses pelatihan memberikan bobot yang terlalu besar kepada `sqm` dibandingkan dengan fitur-fitur lainnya, kita akan menambahkan lapisan normalisasi untuk fitur `sqm`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembentukan Data Train Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data X y\n",
    "Data X yang akan dipergunakan sebagai fitur adalah semua kolom pada `data_model` terkecuali kolom `Rerata Penjualan Normal` yang akan menjadi Data y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pelatihan = data_final.copy()\n",
    "\n",
    "y = data_pelatihan[\"Rerata Penjualan Normal\"]\n",
    "X = data_pelatihan.drop(\"Rerata Penjualan Normal\", axis=1)\n",
    "\n",
    "print(\"Data X:\")\n",
    "print(X.to_string())\n",
    "print(\"\\nData y:\")\n",
    "print(y.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisasi dan Transformasi pada X_train dan X_test\n",
    "Mengingat bahwa pada data `X_train` dan `X_test` kita hanya memiliki 1 fitur numerik (`sqm`) dan sisanya adalah *categorical encoding* maka skala antar fitur bisa terlihat sangat jomplang dan dapat menyebabkan model untuk membutuhkan waktu dalam dalam melakukan pemutakhiran bobot dalam proses *backpropagation* setiap epoch menggunakan optimizer-nya, maka dirasa perlu untuk melakukan normalisasi pada data `X_train` dan `X_test` untuk fitur `sqm`.\n",
    "Serta dikarenakan jumlah sample yang sangat terbatas serta beberapa fitur *categorical encoded* yang memiliki kecondongan (*skewedness*) terhadap beberapa kategori saja, maka juga dirasa perlu untuk melakukan transformasi kuantil (menggunakan *quantile transformer*) pada data `X_train` dan `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"sqm\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi sqm pada Data X_train dan X_test\n",
    "kolom_target_normalisasi = [\"sqm\"]\n",
    "sc = StandardScaler()\n",
    "X_norm = X.copy()\n",
    "X_norm[kolom_target_normalisasi] = sc.fit_transform(X_norm[kolom_target_normalisasi]) # type: ignore\n",
    "\n",
    "print (\"Data X sqm setelah normalisasi:\")\n",
    "print(X_norm[\"sqm\"]) # type: ignore\n",
    "\n",
    "# Menyimpan normalisasi yang sudah di fit dengan X_train\n",
    "dump(sc, 'normalizer/sqm_normalizer.bin', compress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Train Test\n",
    "Pembagian data train dan test adalah dengan rasio data test sebesar 0.3 dari total data, menggunakan random_state yang di-set pada 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.3, random_state=11, shuffle=True)\n",
    "print(f\"X_train:\\n{X_train.to_string()}\\n\") # type: ignore\n",
    "print(f\"y_train:\\n{y_train.to_string()}\\n\") # type: ignore\n",
    "print(f\"X_test:\\n{X_test.to_string()}\\n\") # type: ignore\n",
    "print(f\"y_test:\\n{y_test.to_string()}\") # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting nilai sebenarnya dan prediksi\n",
    "def plot_hasil(ax,\n",
    "               prediksi,\n",
    "               y_test,\n",
    "               label_prediksi,\n",
    "               label_y_test):\n",
    "  ax.scatter(prediksi, y_test, c='crimson')\n",
    "  ax.set_title(\"Mean Absolute Error\")\n",
    "  ax.set_yscale('log')\n",
    "  ax.set_xscale('log')\n",
    "  titik_awal = max(max(prediksi), max(y_test))\n",
    "  titik_akhir = min(min(prediksi), min(y_test))\n",
    "  ax.plot([titik_awal, titik_akhir], [titik_awal, titik_akhir], 'b-')\n",
    "  ax.set_xlabel(label_prediksi)\n",
    "  ax.set_ylabel(label_y_test)\n",
    "  return ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembentukan Model\n",
    "Pada bagian ini kita akan coba untuk membuat beberapa model yang akan dipergunakan dalam pelatihan nantinya. Pelatihan model akan dilakukan menggunakan modul regressor pada TensorFlow, Scikit, LGBM dan XGBoost.\n",
    "\n",
    "#### TensorFlow\n",
    "Beberapa model pelatihan jaringan saraf tiruan akan dilakukan menggunakan TensorFlow. Parameter dasar TensorFlow yang digunakan dalam kasus ini diantaranya:\n",
    "- `loss` yang akan dimonitor adalah *mean absolute error* (terutama `val_mean_absolute_error` pada *callbacks*) pada model yang merupakan formula $\\frac{\\sum^n_{i=1}|\\hat{y}-y_i|}{n}$ dimana $\\hat{y}$ adalah nilai prediksi, $y_i$ adalah nilai sebenarnya dan $n$ adalah jumlah sampel dalam dataset validasi.\n",
    "- `optimizer` yang digunakan dalam melakukan update bobot dan bias pada *neuron* dalam masing-masing *layer* dalam proses *backpropagation* di setiap *epoch* (atau dalam kasus ini pada setiap *batch*) adalah `Adam()` ([*Adaptive Moment Estimation*](https://arxiv.org/abs/1412.6980)) dengan *learning rate* 0.1.\n",
    "- `metric` yang digunakan sama dengan `loss` yaitu `[\"mean_absolute_error\"]`.\n",
    "- Jumlah `epoch` untuk setiap model adalah 1000.\n",
    "- Jumlah `batch_size` yang dipergunakan dalam proses `fit()` untuk mengupdate bobot dan bias pada *neuron* dalam masing-masing layer adalah 4.\n",
    "- `callbacks` yang digunakan adalah:\n",
    "  - `EarlyStopping()` dengan *patience* default 10, namun dalam studi kasus akan menggunakan *patience* 20 dan `monitor` yang diset untuk `val_mean_absolute_error`.\n",
    "  - `ModelCheckpoint()` yang akan memonitor `val_mean_absolute_error` pada setiap *epoch* dan menyimpannya di folder `model/model.name`\n",
    "- Terdapat setidaknya 2 model dasar, yaitu `model_dense_1_layer` dan `model_dnn_3_layer`. `model_dense_1_layer` hanya menggunakan 1 *dense layer* dengan fungsi aktivasi default `Linear`, sedangkan `model_dnn_3_layer` merupakan model dengan 3 *dense layer* yang terdiri dari 2 *dense layer* dengan fungsi aktivasi `ReLU` ([*Rectified Linear Unit*](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)) dan jumlah *neuron* default 64 (dapat disetting berbeda) serta 1 *dense layer* (output) dengan fungsi aktivasi `Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## TENSORFLOW MODEL\n",
    "class TensorFlow:\n",
    "  # Inisiasi kelas dan parameter model\n",
    "  def __init__(self, \n",
    "               es_patience: int = 0,\n",
    "               callbacks: list = [],\n",
    "               loss: Literal[\"mae\", \"mse\"] = \"mae\",\n",
    "               optimizer: Literal[\"adam\", \"sgd\", \"adadelta\"] = \"adam\",\n",
    "               optimizer_lr: float = 0.1,\n",
    "               metric: list = ['mean_absolute_error'],\n",
    "               epoch: int = 100,\n",
    "               verbose_mode: int = 0):\n",
    "    self.es_patience = es_patience\n",
    "    self.callbacks = callbacks\n",
    "    self.optimizer_lr = optimizer_lr\n",
    "    self.metric = metric\n",
    "    self.epoch = epoch\n",
    "    match loss:\n",
    "      case \"mse\":\n",
    "        self.loss = tf.keras.losses.mse\n",
    "      case _:\n",
    "        self.loss = tf.keras.losses.mae\n",
    "    match optimizer:\n",
    "      case \"sgd\":\n",
    "        self.optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=self.optimizer_lr)\n",
    "      case \"adadelta\":\n",
    "        self.optimizer = tf.keras.optimizers.legacy.Adadelta(learning_rate=self.optimizer_lr)\n",
    "      case _:\n",
    "        self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=self.optimizer_lr)\n",
    "    self.verbose_mode = verbose_mode\n",
    "    \n",
    "  # Setting callbacks\n",
    "  def callbacks_model(self, model_compiled):\n",
    "    if len(self.callbacks) == 0:\n",
    "      return [tf.keras.callbacks.EarlyStopping(monitor=\"val_mean_absolute_error\", \n",
    "                                              patience=10 if self.es_patience <= 0 else self.es_patience,\n",
    "                                              restore_best_weights=True,\n",
    "                                              verbose=0),\n",
    "              tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(os.getcwd(), f\"model/{model_compiled.name}\"),\n",
    "                                                monitor=\"val_mean_absolute_error\",\n",
    "                                                save_best_only=True,\n",
    "                                                verbose=0)]\n",
    "    else:\n",
    "      return self.callbacks\n",
    "          \n",
    "  # Compile model\n",
    "  def compile_model(self, model):\n",
    "    model.compile(loss=self.loss,\n",
    "                  optimizer=self.optimizer,\n",
    "                  metrics=self.metric)\n",
    "    \n",
    "  # Fit model\n",
    "  def fit_model(self, X_train, y_train, X_test, y_test, model):\n",
    "    return model.fit(X_train, \n",
    "                     y_train, \n",
    "                     epochs=self.epoch, \n",
    "                     validation_data=(X_test, y_test), \n",
    "                     callbacks=self.callbacks_model(model), \n",
    "                     verbose=self.verbose_mode,\n",
    "                     batch_size=4)\n",
    "  \n",
    "  # Model_Regresi_linear_1_Layer\n",
    "  def model_dense_1_layer(self,\n",
    "                          nama_model: str = \"\"):\n",
    "    # Model Def\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(1, input_dim=X_train.shape[1]) # type: ignore\n",
    "    ], name=nama_model)\n",
    "    self.compile_model(model)\n",
    "    return model\n",
    "  \n",
    "  # Model_Regresi_Linear_3_Layer_2_RELU\n",
    "  def model_dnn_3_layer(self, \n",
    "                        unit_1: int = 64, \n",
    "                        unit_2: int = 64,\n",
    "                        nama_model: str = \"\"):\n",
    "    # Model Def\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(unit_1, activation=\"relu\", name=f\"dense_layer_1_{unit_1}_nodes_relu_activation\"),\n",
    "      tf.keras.layers.Dense(unit_2, activation=\"relu\", name=f\"dense_layer_2_{unit_2}_nodes_relu_activation\"),\n",
    "      tf.keras.layers.Dense(1, name=\"dense_layer_output_1_node_linear_activation\")\n",
    "    ], name=nama_model)\n",
    "    self.compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow dengan Cross Validation\n",
    "Mengingat sampel yang sangat terbatas jumlahnya dalam dataset, kita juga akan mencoba untuk melakukan teknik [*cross-validation*](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) untuk menangani jumlah sampel yang amat sedikit ini. Jenis *cross-validation* yang akan digunakan dalam pelatihan model adalah *K-Fold Cross Validation* dan *Leave-One-Out Cross Validation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFlowCV():\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "  \n",
    "  def callback(self, teknik_fold: str, fold_ke: int, model: tf.keras.Model, es_patience: int = 10):\n",
    "    return [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor=\"val_mean_absolute_error\", \n",
    "                                       patience=es_patience,\n",
    "                                       restore_best_weights=True,\n",
    "                                       verbose=0),\n",
    "      tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(os.getcwd(), f\"model/{teknik_fold}_n_{fold_ke}_{model.name}\"),\n",
    "                                         monitor=\"val_mean_absolute_error\",\n",
    "                                         save_best_only=True,\n",
    "                                         verbose=0)]\n",
    "    \n",
    "  def fit_model(self, model, X, y, validation_data: tuple, epoch: int, callbacks: list, verbose: Literal[0, 1, 2]):\n",
    "    hasil = model.fit(x=X,\n",
    "                      y=y,\n",
    "                      epochs=epoch,\n",
    "                      validation_data=validation_data,\n",
    "                      callbacks=callbacks,\n",
    "                      verbose=verbose)\n",
    "    return hasil\n",
    "  \n",
    "  def ekstrak_hasil(self, output_training, prefix_judul):\n",
    "    # List parameter model\n",
    "    output_param_model = []\n",
    "\n",
    "    # Loop output_training\n",
    "    if len(output_training) != 0:\n",
    "      for index, output in enumerate(output_training):    \n",
    "        # newline\n",
    "        if index != 0:\n",
    "          print(\"\\n\")\n",
    "        # y_test\n",
    "        if \"y_test\" in output:\n",
    "          y_test = output[\"y_test\"]\n",
    "        # Subplot\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        fig.set_figwidth(20)\n",
    "        fig.set_figheight(3)\n",
    "        fig.suptitle(f\"{prefix_judul}_n_{index+1}_{output['model'].name}\")\n",
    "        # Subplot 1 Kurva Loss\n",
    "        pd.DataFrame(output['hasil'].history).plot(ax=ax[0])\n",
    "        ax[0].set_title(\"Kurva Loss\")\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        # Subplot 2 Mean Absolute Error pada Validation Dataset\n",
    "        plot_hasil(ax[1], \n",
    "                  output['prediksi'],\n",
    "                  y_test.tolist(), # type: ignore\n",
    "                  \"Rerata Penjualan Normal\\n(prediksi)\",\n",
    "                  \"Rerata Penjualan Normal\\n(nilai sebenarnya)\")\n",
    "        plt.show()\n",
    "        # Struktur model\n",
    "        output['model'].summary()\n",
    "        # Parameter output model\n",
    "        r2 = r2_score(y_test.tolist(), output['prediksi']) # type: ignore\n",
    "        mae = output['evaluasi']['mean_absolute_error']\n",
    "        mse = mean_squared_error(y_test.tolist(), output['prediksi']) # type: ignore\n",
    "        # Print parameter model\n",
    "        print(f\"Skor R2: {'{:,.2%}'.format(r2) if r2 is not None else None}\")\n",
    "        print(f\"Mean Absolute Error: {'{:,.0f}'.format(mae)}\")\n",
    "        print(f\"Mean Squared Error: {'{:,.0f}'.format(mse)}\")\n",
    "        # Simpan parameter output model dalam dictionary\n",
    "        dict_param_model = {\n",
    "          \"Nama Model\": f\"{(prefix_judul + '_n_' + str(index+1) +'_') if prefix_judul is not None else ''}{output['model'].name}\",\n",
    "          \"Skor R2\": r2,\n",
    "          \"Mean Absolute Error\": mae,\n",
    "          \"Mean Squared Error\": mse\n",
    "        }\n",
    "        output_param_model.append(dict_param_model)\n",
    "        \n",
    "      print(output_param_model) \n",
    "      return output_param_model\n",
    "  \n",
    "  def kfold_cv(self, model: tf.keras.Model, jumlah_split: int = 13, epoch: int = 10, es_patience: int = 10):\n",
    "    # K-Fold split\n",
    "    kfold = KFold(n_splits=jumlah_split, shuffle=True)\n",
    "    # list untuk menyimpan hasil pada setiap fold\n",
    "    list_output_training_kfold = []\n",
    "    \n",
    "    # Loop dalam fold\n",
    "    for fold, (train_indeks, test_indeks) in enumerate(kfold.split(self.X, self.y)):\n",
    "      print(f\"Fold ke {fold + 1} dari {jumlah_split}:\")\n",
    "      # Train dan test untuk fold ini\n",
    "      X_train, y_train = self.X.iloc[train_indeks], self.y.iloc[train_indeks]\n",
    "      X_test, y_test = self.X.iloc[test_indeks], self.y.iloc[test_indeks]\n",
    "      # Fit model\n",
    "      hasil = self.fit_model(\n",
    "        model=model, \n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        validation_data=(X_test, y_test), \n",
    "        epoch=epoch, \n",
    "        callbacks=self.callback(teknik_fold=\"kfold\", \n",
    "                                fold_ke=fold + 1, \n",
    "                                model=model, \n",
    "                                es_patience=es_patience), \n",
    "        verbose=0)\n",
    "      # Output training\n",
    "      evaluasi = model.evaluate(X_test, y_test, verbose=0, return_dict=True) # type: ignore\n",
    "      prediksi = list(np.concatenate(model.predict(X_test, verbose=0)).flat) # type: ignore\n",
    "      dict_output_model = {\n",
    "        \"model\": model,\n",
    "        \"hasil\": hasil,\n",
    "        \"evaluasi\": evaluasi,\n",
    "        \"prediksi\": prediksi,\n",
    "        \"y_test\": y_test\n",
    "      }\n",
    "      print(dict_output_model)\n",
    "      list_output_training_kfold.append(dict_output_model)\n",
    "    \n",
    "    # Output model\n",
    "    output_kfold = self.ekstrak_hasil(list_output_training_kfold, \"kfold\")\n",
    "    \n",
    "    # return model fit\n",
    "    return output_kfold, list_output_training_kfold\n",
    "    \n",
    "  def loo_cv(self, model: tf.keras.Model, epoch: int = 10, es_patience: int = 10):\n",
    "    # LOOCV\n",
    "    loo = LeaveOneOut()\n",
    "    # list untuk menyimpan hasil pada setiap fold\n",
    "    list_output_training_loo = []\n",
    "    \n",
    "    # Loop dalam fold\n",
    "    for fold, (train_indeks, test_indeks) in enumerate(loo.split(self.X)):\n",
    "      print(f\"Fold ke {fold + 1} dari {len(X)}:\")\n",
    "      # Train dan test untuk fold ini\n",
    "      X_train, y_train = self.X.iloc[train_indeks], self.y.iloc[train_indeks]\n",
    "      X_test, y_test = self.X.iloc[test_indeks], self.y.iloc[test_indeks]\n",
    "      # Fit model\n",
    "      hasil = self.fit_model(\n",
    "        model=model,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epoch=epoch,\n",
    "        callbacks=self.callback(teknik_fold=\"leaveoneout\",\n",
    "                                fold_ke=fold + 1,\n",
    "                                model=model,\n",
    "                                es_patience=es_patience),\n",
    "        verbose=0\n",
    "      )\n",
    "      # Output training\n",
    "      evaluasi = model.evaluate(X_test, y_test, verbose=0, return_dict=True) # type: ignore\n",
    "      prediksi = list(np.concatenate(model.predict(X_test, verbose=0)).flat) # type: ignore\n",
    "      dict_output_model = {\n",
    "        \"model\": model,\n",
    "        \"hasil\": hasil,\n",
    "        \"evaluasi\": evaluasi,\n",
    "        \"prediksi\": prediksi,\n",
    "        \"y_test\": y_test\n",
    "      }\n",
    "      print(dict_output_model)\n",
    "      list_output_training_loo.append(dict_output_model)\n",
    "      \n",
    "    # output model\n",
    "    output_loo = self.ekstrak_hasil(list_output_training_loo, \"leaveoneout\")\n",
    "    \n",
    "    # return model fit\n",
    "    return output_loo, list_output_training_loo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit\n",
    "Fungsi regresi linear yang akan dipergunakan dari modul Scikit adalah `LinearRegression()` dan `DecisionTreeRegressor()`. Tidak banyak kustomisasi yang dapat dilakukan pada modul ini karena sifat *built-in* dari fungsi-fungsi tersebut secara default dinilai sudah cukup baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCIKIT MODEL\n",
    "class SK:\n",
    "  # Inisiasi kelas dan parameter model\n",
    "  def __init__(self, \n",
    "               fitur_train: pd.DataFrame, \n",
    "               target_train: pd.DataFrame,\n",
    "               fitur_test: pd.DataFrame,\n",
    "               target_test: pd.DataFrame):\n",
    "    self.fitur_train = fitur_train\n",
    "    self.target_train = target_train\n",
    "    self.fitur_test = fitur_test\n",
    "    self.target_test = target_test\n",
    "    \n",
    "  # Fit model\n",
    "  def fit_model(self, model):\n",
    "    return model.fit(self.fitur_train, self.target_train)\n",
    "  \n",
    "  # Plotting nilai sebenarnya dan prediksi\n",
    "  def plot_hasil(self):\n",
    "    return\n",
    "  \n",
    "  # Model_SK_Linear_Regresi\n",
    "  def regresi_linear(self):\n",
    "    return LinearRegression()\n",
    "  \n",
    "  # Model_SK_Decision_Tree\n",
    "  def decision_tree(self):\n",
    "    return DecisionTreeRegressor()\n",
    "  \n",
    "  # Model_SK_Random_Forest\n",
    "  def random_forest(self):\n",
    "    return RandomForestRegressor()\n",
    "  \n",
    "  # Model_SK_Gradient_Boosting\n",
    "  def gradient_boosting(self):\n",
    "    return GradientBoostingRegressor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM dan XGBoost\n",
    "\n",
    "Dua pustaka pembelajaran mesin lainnya yang akan digunakan diantaranya *light gradient-boosting machine* ([LightGBM](https://lightgbm.readthedocs.io))dan *extreme gradient boosting* ([XGBoost](https://xgboost.readthedocs.io)). Sama dengan Scikit, tidak banyak kustomisasi yang dilakukan untuk parameter di dalam dua pustaka ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM DAN XGBOOST MODEL (ENSEMBLE REGRESSOR)\n",
    "class EnsembleModel:\n",
    "  def __init__(self,\n",
    "               fitur_train: pd.DataFrame,\n",
    "               target_train: pd.DataFrame,\n",
    "               fitur_test: pd.DataFrame,\n",
    "               target_test: pd.DataFrame):\n",
    "    self.fitur_train = fitur_train\n",
    "    self.target_train = target_train\n",
    "    self.fitur_test = fitur_test\n",
    "    self.target_test = target_test\n",
    "  \n",
    "  def fit_model(self, model):\n",
    "    return model.fit(self.fitur_train, self.target_train)\n",
    "  \n",
    "  def lgbm(self):\n",
    "    return lightgbm.LGBMRegressor()\n",
    "  \n",
    "  def xgb(self):\n",
    "    return xgboost.XGBRFRegressor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pelatihan Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow\n",
    "Kita akan melakukan pelatihan jaringan saraf tiruan menggunakan TensorFlow untuk setidaknya 7 model, yaitu:\n",
    "- Model DNN 3 Layer ReLU 16 16 (`Model_DNN_3_Layer_RELU_16_16`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 16 *neuron*.\n",
    "- Model DNN 3 Layer ReLU 32 16 (`Model_DNN_3_Layer_RELU_32_16`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 32 dan 16 *neuron* secara berturut-turut.\n",
    "- Model DNN 3 Layer ReLU 32 32 (`Model_DNN_3_Layer_RELU_32_32`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 32 *neuron*.\n",
    "- Model DNN 3 Layer ReLU 64 32 (`Model_DNN_3_Layer_RELU_64_32`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 64 dan 32 *neuron* secara berturut-turut.\n",
    "- Model DNN 3 Layer ReLU 64 64 (`Model_DNN_3_Layer_RELU_64_64`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 64 *neuron* atau nilai default `unit_size` pada `model_dnn_3_layer`.\n",
    "- Model DNN 3 Layer ReLU 128 64 (`Model_DNN_3_Layer_RELU_128_64`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 128 dan 64 *neuron* secara berturut-turut.\n",
    "- Model DNN 3 Layer ReLU 128 128 (`Model_DNN_3_Layer_RELU_128_128`) dengan jumlah *neuron* pada *dense layer* pertama dan kedua masing-masing 128 *neuron*.  \n",
    "  \n",
    "Parameter `patience` dari `tf.keras.callbacks.EarlyStopping()` di-set menjadi 20 *epoch* dan jumlah `epoch` yang digunakan dalam setiap pelatihan adalah 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow = TensorFlow(\n",
    "  es_patience=20,\n",
    "  epoch=1000\n",
    ")\n",
    "\n",
    "# List model yang di-compile\n",
    "tensorflow_model = [tensorflow.model_dnn_3_layer(16, 16, \"Model_DNN_3_Layer_RELU_16_16\"),\n",
    "                    tensorflow.model_dnn_3_layer(32, 16, \"Model_DNN_3_Layer_RELU_32_16\"),\n",
    "                    tensorflow.model_dnn_3_layer(32, 32, \"Model_DNN_3_Layer_RELU_32_32\"), \n",
    "                    tensorflow.model_dnn_3_layer(64, 32, \"Model_DNN_3_Layer_RELU_64_32\"), \n",
    "                    tensorflow.model_dnn_3_layer(nama_model=\"Model_DNN_3_Layer_RELU_64_64\"), \n",
    "                    tensorflow.model_dnn_3_layer(128, 64, \"Model_DNN_3_Layer_RELU_128_64\"), \n",
    "                    tensorflow.model_dnn_3_layer(128, 128, \"Model_DNN_3_Layer_RELU_128_128\")]\n",
    "\n",
    "# List output pelatihan model tensorflow\n",
    "output_training = []\n",
    "\n",
    "# Loop fit dan simpan model\n",
    "for index, model in enumerate(tensorflow_model):\n",
    "  # Fit setiap model dan simpan di variabel hasil\n",
    "  hasil = tensorflow.fit_model(X_train, y_train, X_test, y_test, model)\n",
    "  # Simpan evaluasi model\n",
    "  evaluasi = model.evaluate(X_test, y_test, verbose=0, return_dict=True) # type: ignore\n",
    "  # Lakukan prediksi model\n",
    "  prediksi = list(np.concatenate(model.predict(X_test, verbose=0)).flat) # type: ignore\n",
    "  dict_output_model = {\n",
    "    \"model\": model,\n",
    "    \"hasil\": hasil,\n",
    "    \"evaluasi\": evaluasi,\n",
    "    \"prediksi\": prediksi\n",
    "    }\n",
    "  print(dict_output_model)\n",
    "  output_training.append(dict_output_model)\n",
    "  \n",
    "# List parameter model\n",
    "output_param_model = []\n",
    "\n",
    "# Loop output_training\n",
    "if len(output_training) != 0:\n",
    "  for index, output in enumerate(output_training):    \n",
    "    # newline\n",
    "    if index != 0:\n",
    "      print(\"\\n\")\n",
    "    # Subplot\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(3)\n",
    "    fig.suptitle(f\"{output['model'].name}\")\n",
    "    # Subplot 1 Kurva Loss\n",
    "    pd.DataFrame(output['hasil'].history).plot(ax=ax[0])\n",
    "    ax[0].set_title(\"Kurva Loss\")\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    # Subplot 2 Mean Absolute Error pada Validation Dataset\n",
    "    plot_hasil(ax[1], \n",
    "              output['prediksi'],\n",
    "              y_test.tolist(), # type: ignore\n",
    "              \"Rerata Penjualan Normal\\n(prediksi)\",\n",
    "              \"Rerata Penjualan Normal\\n(nilai sebenarnya)\")\n",
    "    plt.show()\n",
    "    # Struktur model\n",
    "    output['model'].summary()\n",
    "    # Parameter output model\n",
    "    r2 = r2_score(y_test.tolist(), output['prediksi']) # type: ignore\n",
    "    mae = output['evaluasi']['mean_absolute_error']\n",
    "    mse = mean_squared_error(y_test.tolist(), output['prediksi']) # type: ignore\n",
    "    # Print parameter model\n",
    "    print(f\"Skor R2: {'{:,.2%}'.format(r2)}\")\n",
    "    print(f\"Mean Absolute Error: {'{:,.0f}'.format(mae)}\")\n",
    "    print(f\"Mean Squared Error: {'{:,.0f}'.format(mse)}\")\n",
    "    # Simpan parameter output model dalam dictionary\n",
    "    dict_param_model = {\n",
    "      \"Nama Model\": output['model'].name,\n",
    "      \"Skor R2\": r2,\n",
    "      \"Mean Absolute Error\": mae,\n",
    "      \"Mean Squared Error\": mse\n",
    "    }\n",
    "    output_param_model.append(dict_param_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ringkasan Pelatihan Model TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat nama kolom\n",
    "list_nama_kolom = ['Skor R2', 'Mean Absolute Error', 'Mean Squared Error']\n",
    "# Ekstrak nama model dari output_param_model\n",
    "list_nama_model = [model['Nama Model'] for model in output_param_model]\n",
    "# Ekstrak nilai output param masing - masing model\n",
    "# Loop berdasar nama model\n",
    "list_parameter_model = []\n",
    "for indeks in range(len(list_nama_model)):\n",
    "  # ambil r2, mae dan mse\n",
    "  list_parameter_model.append([\n",
    "    output_param_model[indeks]['Skor R2'],\n",
    "    output_param_model[indeks]['Mean Absolute Error'],\n",
    "    output_param_model[indeks]['Mean Squared Error']\n",
    "    ])\n",
    "# Buat dataframe ringkasan\n",
    "df_ringkasan = pd.DataFrame(list_parameter_model, \n",
    "                             columns=list_nama_kolom, \n",
    "                             index=list_nama_model)\n",
    "\n",
    "# Print model terbaik untuk masing - masing kategori\n",
    "r2_terbaik, mae_terbaik, mse_terbaik = (\n",
    "  df_ringkasan[list_nama_kolom[0]].idxmax(),\n",
    "  df_ringkasan[list_nama_kolom[1]].idxmin(),\n",
    "  df_ringkasan[list_nama_kolom[2]].idxmin()\n",
    "  )\n",
    "print(f\"Model dengan {list_nama_kolom[0]} terbaik:\\t\\t\\t\\t{r2_terbaik}\")\n",
    "print(f\"Model dengan {list_nama_kolom[1]} terbaik:\\t\\t{mae_terbaik}\")\n",
    "print(f\"Model dengan {list_nama_kolom[2]} terbaik:\\t\\t{mse_terbaik}\\n\")\n",
    "\n",
    "# Formatting dataframe dan tampilkan df_ringkasan\n",
    "df_ringkasan[list_nama_kolom[0]] = df_ringkasan[list_nama_kolom[0]].map('{:,.2%}'.format)\n",
    "df_ringkasan[list_nama_kolom[1]] = df_ringkasan[list_nama_kolom[1]].map('{:,.0f}'.format)\n",
    "df_ringkasan[list_nama_kolom[2]] = df_ringkasan[list_nama_kolom[2]].map('{:,.0f}'.format)\n",
    "print(df_ringkasan.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow dengan Cross Validation\n",
    "Pada bagian ini kita akan melakukan pelatihan lebih lanjut untuk model terbaik dari pelatihan tensorflow sebelumnya. Kali ini kita akan menggunakan teknik *cross validation* untuk mengatasi jumlah sampel dalam dataset yang sangat terbatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih model tensorflow terbaik sebelumnya\n",
    "indeks_model_terbaik = None\n",
    "for indeks, baris in enumerate(df_ringkasan.index):\n",
    "  if baris == mae_terbaik:\n",
    "    indeks_model_terbaik = indeks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation pada Model Terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if indeks_model_terbaik is not None:\n",
    "  tensorflow_cv = TensorFlowCV(X_norm, y)\n",
    "  model_terbaik = tensorflow_model[indeks_model_terbaik]\n",
    "  output_param_kfold, hasil_kfold = tensorflow_cv.kfold_cv(model=model_terbaik,\n",
    "                                                           epoch=1000,\n",
    "                                                           es_patience=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-One-Out Cross Validation pada Model Terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if indeks_model_terbaik is not None:\n",
    "  output_param_loo, hasil_loo = tensorflow_cv.loo_cv(model=model_terbaik,\n",
    "                                                     epoch=1000,\n",
    "                                                     es_patience=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = SK(fitur_train=X_train, # type: ignore\n",
    "        target_train=y_train, # type: ignore\n",
    "        fitur_test=X_test, # type: ignore\n",
    "        target_test=y_test) # type: ignore\n",
    "sk_model = [sk.regresi_linear(), sk.decision_tree(), sk.random_forest(), sk.gradient_boosting()]\n",
    "# Nama Model\n",
    "list_nama_model = [\n",
    "  'Linear Regression',\n",
    "  'Decision Tree Regressor',\n",
    "  'Random Forest Regressor',\n",
    "  'Gradient Boosting Regressor'\n",
    "  ]\n",
    "for index, model in enumerate(sk_model):\n",
    "  hasil = sk.fit_model(model)\n",
    "  prediksi = hasil.predict(X_test)\n",
    "  r2 = r2_score(y_test, prediksi)\n",
    "  mae = mean_absolute_error(y_test, prediksi)\n",
    "  mse = mean_squared_error(y_test, prediksi)\n",
    "  fig, ax = plt.subplots()\n",
    "  plot_hasil(ax,\n",
    "             prediksi,\n",
    "             y_test.tolist(), # type: ignore\n",
    "             \"Rerata Penjualan Normal\\n(prediksi)\",\n",
    "             \"Rerata Penjualan Normal\\n(nilai sebenarnya)\")\n",
    "  fig.suptitle(list_nama_model[index])\n",
    "  plt.show()\n",
    "  print(f\"Skor R2: {'{:,.2%}'.format(r2)}\")\n",
    "  print(f\"Mean Absolute Error: {'{:,.0f}'.format(mae)}\")\n",
    "  print(f\"Mean Squared Error: {'{:,.0f}'.format(mse)}\")\n",
    "  # Simpan parameter output model dalam dictionary\n",
    "  dict_param_model = {\n",
    "    \"Nama Model\": list_nama_model[index],\n",
    "    \"Skor R2\": r2,\n",
    "    \"Mean Absolute Error\": mae,\n",
    "    \"Mean Squared Error\": mse\n",
    "  }\n",
    "  output_param_model.append(dict_param_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM dan XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleModel(fitur_train=X_train, # type: ignore\n",
    "                         target_train=y_train, # type: ignore\n",
    "                         fitur_test=X_test, # type: ignore\n",
    "                         target_test=y_test) # type: ignore\n",
    "ensemble_model = [ensemble.lgbm(), ensemble.xgb()]\n",
    "# Nama Model\n",
    "list_nama_model = [\n",
    "  'LightGBM',\n",
    "  'XGBoost'\n",
    "  ]\n",
    "for index, model in enumerate(ensemble_model):\n",
    "  hasil = ensemble.fit_model(model)\n",
    "  prediksi = hasil.predict(X_test)\n",
    "  r2 = r2_score(y_test, prediksi)\n",
    "  mae = mean_absolute_error(y_test, prediksi)\n",
    "  mse = mean_squared_error(y_test, prediksi)\n",
    "  fig, ax = plt.subplots()\n",
    "  plot_hasil(ax,\n",
    "             prediksi,\n",
    "             y_test.tolist(), # type: ignore\n",
    "             \"Rerata Penjualan Normal\\n(prediksi)\",\n",
    "             \"Rerata Penjualan Normal\\n(nilai sebenarnya)\")\n",
    "  fig.suptitle(list_nama_model[index])\n",
    "  plt.show()\n",
    "  print(f\"Skor R2: {'{:,.2%}'.format(r2)}\")\n",
    "  print(f\"Mean Absolute Error: {'{:,.0f}'.format(mae)}\")\n",
    "  print(f\"Mean Squared Error: {'{:,.0f}'.format(mse)}\")\n",
    "  # Simpan parameter output model dalam dictionary\n",
    "  dict_param_model = {\n",
    "    \"Nama Model\": \"LightGBM\" if index == 0 else \"XGBoost\",\n",
    "    \"Skor R2\": r2,\n",
    "    \"Mean Absolute Error\": mae,\n",
    "    \"Mean Squared Error\": mse\n",
    "  }\n",
    "  output_param_model.append(dict_param_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat nama kolom\n",
    "list_nama_kolom = ['Skor R2', 'Mean Absolute Error', 'Mean Squared Error']\n",
    "# list output param model\n",
    "list_out_param_model = [output_param_model, output_param_kfold, output_param_loo]\n",
    "# Ekstrak nama model dan parameter model dari list_out_param_model\n",
    "list_nama_model = []\n",
    "list_parameter_model = []\n",
    "for list_param in list_out_param_model:\n",
    "  for model in list_param:\n",
    "    list_nama_model.append(model['Nama Model'])\n",
    "    list_parameter_model.append([\n",
    "      model['Skor R2'],\n",
    "      model['Mean Absolute Error'],\n",
    "      model['Mean Squared Error']\n",
    "    ])\n",
    "# Buat dataframe kesimpulan\n",
    "df_kesimpulan = pd.DataFrame(list_parameter_model,\n",
    "                             columns=list_nama_kolom,\n",
    "                             index=list_nama_model)\n",
    "# Print model terbaik untuk masing - masing kategori\n",
    "r2_terbaik, mae_terbaik, mse_terbaik = (\n",
    "  df_kesimpulan[list_nama_kolom[0]].idxmax(),\n",
    "  df_kesimpulan[list_nama_kolom[1]].idxmin(),\n",
    "  df_kesimpulan[list_nama_kolom[2]].idxmin()\n",
    "  )\n",
    "print(f\"Model dengan {list_nama_kolom[0]} terbaik:\\t\\t\\t\\t{r2_terbaik}\")\n",
    "print(f\"Model dengan {list_nama_kolom[1]} terbaik:\\t\\t{mae_terbaik}\")\n",
    "print(f\"Model dengan {list_nama_kolom[2]} terbaik:\\t\\t{mse_terbaik}\\n\")\n",
    "\n",
    "# Formatting dataframe dan tampilkan df_kesimpulan\n",
    "df_kesimpulan[list_nama_kolom[0]] = df_kesimpulan[list_nama_kolom[0]].map('{:,.2%}'.format)\n",
    "df_kesimpulan[list_nama_kolom[1]] = df_kesimpulan[list_nama_kolom[1]].map('{:,.0f}'.format)\n",
    "df_kesimpulan[list_nama_kolom[2]] = df_kesimpulan[list_nama_kolom[2]].map('{:,.0f}'.format)\n",
    "print(df_kesimpulan.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluasi Model Terbaik pada `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluasi_model(nama_model: str):\n",
    "  print(f\"NAMA MODEL: {nama_model}\")\n",
    "  loaded_model = tf.saved_model.load(os.path.join(os.getcwd(), f\"model/{nama_model}\"))\n",
    "  list_prediksi = []\n",
    "  list_baris = []\n",
    "  for x in X_test.iterrows(): # type: ignore\n",
    "    list_nilai = []\n",
    "    for row in x[1]:\n",
    "      list_nilai.append(row)\n",
    "    list_baris.append(list_nilai)\n",
    "  prediksi = list(np.concatenate(loaded_model(list_baris)).flat) # type: ignore\n",
    "  list_prediksi.append(prediksi)\n",
    "  list_prediksi_final = []\n",
    "  for nilai in list_prediksi[0]:\n",
    "    list_prediksi_final.append([nilai])\n",
    "  df_evaluasi = pd.DataFrame(list_prediksi_final, columns=['Rerata Penjualan Normal (prediksi)'])\n",
    "  y_test.reset_index(drop=True, inplace=True) # type: ignore\n",
    "  df_evaluasi_final = pd.concat([y_test, df_evaluasi], axis=1) # type: ignore\n",
    "  df_evaluasi_final[\"error\"] = df_evaluasi_final[\"Rerata Penjualan Normal (prediksi)\"] - df_evaluasi_final[\"Rerata Penjualan Normal\"]\n",
    "  print(df_evaluasi_final.to_string())\n",
    "  print(\"\")\n",
    "  print(\"Mean Absolute Error pada model dengan dataset y_test:\")\n",
    "  print('{:,.0f}'.format(abs(df_evaluasi_final[\"error\"]).mean()))\n",
    "  \n",
    "list_model_terbaik = [r2_terbaik, mae_terbaik]\n",
    "for indeks, model in enumerate(list_model_terbaik):\n",
    "  if indeks != 0:\n",
    "    print(\"\")\n",
    "  evaluasi_model(model) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil akhir dari setiap model yang sudah melalu proses pelatihan dalam jaringan saraf tiruan diatas, kita dapat menyimpulkan bahwa `Model_DNN_3_Layer_RELU_128_128` pada *cross validation* `LeaveOneOut` n ke 30 (`leaveoneout_n_30_Model_DNN_3_Layer_RELU_128_128`) adalah model dengan performa terbaik diantara model-model dalam pelatihan jaringan saraf tiruan untuk semua kategori parameter evaluasi MAE.\n",
    "\n",
    "Untuk implementasi model ini ke dalam aplikasi ke depannya, kita akan melakukan konversi model terbaik dari format `protobuf (.pb)` menjadi `tensorflow lite (.tlite)` untuk disisipkan pada aplikasi dan melakukan inferensi terhadap input dari user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konverter\n",
    "konverter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(os.getcwd(), f\"model/{mae_terbaik}\"))\n",
    "konverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # type: ignore\n",
    "konverter.allow_custom_ops = True\n",
    "konverter.experimental_new_converter = True\n",
    "tflite_model = konverter.convert()\n",
    "\n",
    "# Simpan model\n",
    "with open('model/aplikasi/model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
